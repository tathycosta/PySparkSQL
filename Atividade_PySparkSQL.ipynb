{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5tuuqP+BnDyuvADvAqsp0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tathycosta/PySparkSQL/blob/main/Atividade_PySparkSQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação da biblioteca pandas\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zJpA3f2CSZQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalação dos requisitos para o PySpark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null #Instala o OpenJDK 8 (necessário para o Spark rodar no ambiente). A opção -qq reduz a saída do comando, e > /dev/null silencia os logs.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz #Baixa a versão 3.1.1 do Apache Spark com suporte ao Hadoop 3.2 do repositório oficial.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz #Descompacta o arquivo .tgz para acesso aos binários do Spark.\n",
        "!pip install -q findspark #findspark é uma biblioteca que facilita a inicialização do Spark em ambientes como o Google Colab."
      ],
      "metadata": {
        "id": "WBDZ9nTlSZGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa o módulo 'os' para interagir com variáveis de ambiente do sistema operacional\n",
        "import os\n",
        "\n",
        "# Define a variável de ambiente JAVA_HOME, indicando o caminho para o Java 8\n",
        "# O Spark precisa do Java para ser executado, e aqui especificamos onde ele está instalado\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Define a variável de ambiente SPARK_HOME, indicando o caminho de instalação do Spark\n",
        "# Isso é necessário para que o Python saiba onde encontrar os binários e bibliotecas do Spark\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "# Importa a biblioteca 'findspark', que ajuda a localizar e configurar o Spark no ambiente Python\n",
        "import findspark\n",
        "\n",
        "# Inicializa o findspark, permitindo que possamos importar e utilizar o PySpark no código\n",
        "findspark.init()\n"
      ],
      "metadata": {
        "id": "Ss1BXDSBSY94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa a classe SparkSession do módulo pyspark.sql\n",
        "# SparkSession é o ponto de entrada principal para trabalhar com DataFrames no PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Inicializar a SparkSession com suporte ao Hive\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark with Hive on Colab\") \\\n",
        "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/content/spark-warehouse\") \\\n",
        "    .config(\"hive.metastore.warehouse.dir\", \"/content/spark-warehouse\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "# Define o nome da aplicação Spark (aparece nos logs)\n",
        "# Configura o Spark para usar o catálogo do Hive\n",
        "# Define o diretório do warehouse para o Hive\n",
        "# Criar diretório para o warehouse\n",
        "# Habilita o suporte ao Hive, permitindo consultas SQL compatíveis com Hive\n",
        "# Cria a SparkSession ou reutiliza uma existente\n",
        "# Cria o diretório para armazenar os metadados e tabelas gerenciadas pelo Hive\n",
        "!mkdir -p /content/spark-warehouse\n",
        "\n"
      ],
      "metadata": {
        "id": "mdNiiygKSY0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica o SparkContext\n",
        "print(spark)\n",
        "\n",
        "# Exibe a Spark version\n",
        "print(spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQOpi-JOSiII",
        "outputId": "fd4b0894-8b9a-4d32-ad32-32a991df16a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f7600a20750>\n",
            "3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aluguel = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vRBm8orBNbKLqHPtSSrZCLJrduyM_lI-4ZfVkmRqkqK7PvqnzkvKV0mJbRCiHH6IYVpcMXeefCqQsW2/pub?gid=0&single=true&output=csv')\n",
        "cliente = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vRBm8orBNbKLqHPtSSrZCLJrduyM_lI-4ZfVkmRqkqK7PvqnzkvKV0mJbRCiHH6IYVpcMXeefCqQsW2/pub?gid=1573858125&single=true&output=csv')\n",
        "carro = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vRBm8orBNbKLqHPtSSrZCLJrduyM_lI-4ZfVkmRqkqK7PvqnzkvKV0mJbRCiHH6IYVpcMXeefCqQsW2/pub?gid=2131324504&single=true&output=csv')\n",
        "marca = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vRBm8orBNbKLqHPtSSrZCLJrduyM_lI-4ZfVkmRqkqK7PvqnzkvKV0mJbRCiHH6IYVpcMXeefCqQsW2/pub?gid=1957306968&single=true&output=csv')\n"
      ],
      "metadata": {
        "id": "Ep5wXhwpS4Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cliente.to_csv('cliente.csv',index=False)\n",
        "aluguel.to_csv('aluguel.csv',index=False)\n",
        "carro.to_csv('carro.csv',index=False)\n",
        "marca.to_csv('marca.csv',index=False)"
      ],
      "metadata": {
        "id": "Ro5HXYhvS59N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('''\n",
        "CREATE TABLE IF NOT EXISTS cliente (\n",
        "  codcliente INT,\n",
        "  nome STRING,\n",
        "  cidade STRING,\n",
        "  sexo STRING,\n",
        "  estado STRING,\n",
        "  estadocivil STRING\n",
        ")\n",
        "\n",
        "USING CSV\n",
        "OPTIONS (path '/content/cliente.csv', header 'true', inferSchema 'true')\n",
        "\n",
        "''')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CbR7bzOipHv",
        "outputId": "d2d4004c-f663-4455-e11e-c62e7f62ef21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('''\n",
        "SELECT * FROM cliente\n",
        "''').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM4YMYoNi6fj",
        "outputId": "0b8af9fe-8285-448c-a9f0-cc3a9063a603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------+---------------+----+------+-----------+\n",
            "|codcliente|            nome|         cidade|sexo|estado|estadocivil|\n",
            "+----------+----------------+---------------+----+------+-----------+\n",
            "|         1|       Ana Silva|Duque de Caxias|   F|    RJ|          C|\n",
            "|         2|   Bruna Pereira|        Niterói|   F|    RJ|          C|\n",
            "|         3|Túlio Nascimento|Duque de Caxias|   M|    RJ|          S|\n",
            "|         4|  Fernando Souza|       Campinas|   M|    SP|          S|\n",
            "|         5|   Lúcia Andrade|      São Paulo|   F|    SP|          C|\n",
            "+----------+----------------+---------------+----+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('''\n",
        "CREATE TABLE IF NOT EXISTS aluguel (\n",
        "  codaluguel INT,\n",
        "  codcliente INT,\n",
        "  codcarro INT,\n",
        "  data_aluguel DATE\n",
        ")\n",
        "\n",
        "USING CSV\n",
        "OPTIONS (path '/content/aluguel.csv', header 'true', inferSchema 'true')\n",
        "\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuDdNEPxirL1",
        "outputId": "178048d7-6dd4-4f25-fbc5-2b61223ccf2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('''\n",
        "SELECT * FROM aluguel\n",
        "''').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YPtFPWqjEnK",
        "outputId": "fe61b592-49c3-4ded-a63d-43d7423d1928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+------------+\n",
            "|codaluguel|codcliente|codcarro|data_aluguel|\n",
            "+----------+----------+--------+------------+\n",
            "|         1|         3|       2|  2023-04-01|\n",
            "|         2|         2|       1|  2023-04-02|\n",
            "|         3|         2|       1|  2023-04-03|\n",
            "|         4|         2|       3|  2023-04-04|\n",
            "|         5|         1|       4|  2023-04-05|\n",
            "|         6|         1|       4|  2023-04-13|\n",
            "|         7|         1|       1|  2023-04-15|\n",
            "|         8|         5|       2|  2023-04-19|\n",
            "|         9|         5|       2|  2023-04-21|\n",
            "|        10|         3|       1|  2023-04-25|\n",
            "+----------+----------+--------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('''\n",
        "CREATE TABLE IF NOT EXISTS carro(\n",
        "    codcarro INT,\n",
        "    codmarca INT,\n",
        "    modelo STRING,\n",
        "    valor DOUBLE\n",
        ")\n",
        "USING CSV\n",
        "OPTIONS (path '/content/carro.csv', header 'true', inferSchema 'true')\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlnsgFl2irDM",
        "outputId": "2d8f85e2-f523-4e6c-b13c-eb35ed47ceae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('''\n",
        "SELECT * FROM carro\n",
        "''').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQwOACn-jJDL",
        "outputId": "b65d09de-00b8-4bac-cff5-d6308da52cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+------+-----+\n",
            "|codcarro|codmarca|modelo|valor|\n",
            "+--------+--------+------+-----+\n",
            "|       1|       1|    Ka|100.0|\n",
            "|       2|       2|  Argo|150.0|\n",
            "|       3|       3|  Onix|170.0|\n",
            "|       4|       4|  Polo|150.0|\n",
            "|       5|       5|  Kwid|120.0|\n",
            "+--------+--------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('''CREATE TABLE IF NOT EXISTS marca (\n",
        "  codmarca INT,\n",
        "  marca STRING\n",
        ")\n",
        "USING CSV\n",
        "OPTIONS (path '/content/marca.csv', header 'true', inferSchema 'true')\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGzitvTjiq5y",
        "outputId": "ebc02fdc-2b96-4842-eb4c-3a1d5058749f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('''\n",
        "SELECT * FROM marca\n",
        "''').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-hfE4etjONk",
        "outputId": "75f001d8-e970-42ca-e39f-06f2835a9175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+\n",
            "|codmarca|     marca|\n",
            "+--------+----------+\n",
            "|       1|      Ford|\n",
            "|       2|      Fiat|\n",
            "|       3| Chevrolet|\n",
            "|       4|Volkswagen|\n",
            "|       5|   Renault|\n",
            "+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "01 - Carregar as quatro tabelas do banco locadora no PySpark como DataFrames"
      ],
      "metadata": {
        "id": "g_-7Qkc1QJ0y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8C5oJ0kP97D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b181c66-1adc-4f9d-e1dd-9fadd4f2d916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clientes:\n",
            "+----------+----------------+---------------+----+------+-----------+\n",
            "|codcliente|            nome|         cidade|sexo|estado|estadocivil|\n",
            "+----------+----------------+---------------+----+------+-----------+\n",
            "|         1|       Ana Silva|Duque de Caxias|   F|    RJ|          C|\n",
            "|         2|   Bruna Pereira|        Niterói|   F|    RJ|          C|\n",
            "|         3|Túlio Nascimento|Duque de Caxias|   M|    RJ|          S|\n",
            "|         4|  Fernando Souza|       Campinas|   M|    SP|          S|\n",
            "|         5|   Lúcia Andrade|      São Paulo|   F|    SP|          C|\n",
            "+----------+----------------+---------------+----+------+-----------+\n",
            "\n",
            "Carros:\n",
            "+--------+--------+------+-----+\n",
            "|codcarro|codmarca|modelo|valor|\n",
            "+--------+--------+------+-----+\n",
            "|       1|       1|    Ka|100.0|\n",
            "|       2|       2|  Argo|150.0|\n",
            "|       3|       3|  Onix|170.0|\n",
            "|       4|       4|  Polo|150.0|\n",
            "|       5|       5|  Kwid|120.0|\n",
            "+--------+--------+------+-----+\n",
            "\n",
            "Marcas:\n",
            "+--------+----------+\n",
            "|codmarca|     marca|\n",
            "+--------+----------+\n",
            "|       1|      Ford|\n",
            "|       2|      Fiat|\n",
            "|       3| Chevrolet|\n",
            "|       4|Volkswagen|\n",
            "|       5|   Renault|\n",
            "+--------+----------+\n",
            "\n",
            "Aluguéis:\n",
            "+----------+----------+--------+------------+\n",
            "|codaluguel|codcliente|codcarro|data_aluguel|\n",
            "+----------+----------+--------+------------+\n",
            "|         1|         3|       2|  2023-04-01|\n",
            "|         2|         2|       1|  2023-04-02|\n",
            "|         3|         2|       1|  2023-04-03|\n",
            "|         4|         2|       3|  2023-04-04|\n",
            "|         5|         1|       4|  2023-04-05|\n",
            "|         6|         1|       4|  2023-04-13|\n",
            "|         7|         1|       1|  2023-04-15|\n",
            "|         8|         5|       2|  2023-04-19|\n",
            "|         9|         5|       2|  2023-04-21|\n",
            "|        10|         3|       1|  2023-04-25|\n",
            "+----------+----------+--------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('''\n",
        "CREATE TABLE IF NOT EXISTS cliente\n",
        "USING csv\n",
        "OPTIONS (\n",
        "    path \"cliente.csv\",\n",
        "    header \"true\",\n",
        "    inferSchema \"true\"\n",
        ")\n",
        "''')\n",
        "\n",
        "spark.sql('''\n",
        "CREATE TABLE IF NOT EXISTS carro\n",
        "USING csv\n",
        "OPTIONS (\n",
        "    path \"carro.csv\",\n",
        "    header \"true\",\n",
        "    inferSchema \"true\"\n",
        ")\n",
        "''')\n",
        "\n",
        "spark.sql('''\n",
        "CREATE TABLE IF NOT EXISTS marca\n",
        "USING csv\n",
        "OPTIONS (\n",
        "    path \"marca.csv\",\n",
        "    header \"true\",\n",
        "    inferSchema \"true\"\n",
        ")\n",
        "''')\n",
        "\n",
        "spark.sql('''\n",
        "CREATE TABLE IF NOT EXISTS aluguel\n",
        "USING csv\n",
        "OPTIONS (\n",
        "    path \"aluguel.csv\",\n",
        "    header \"true\",\n",
        "    inferSchema \"true\"\n",
        ")\n",
        "''')\n",
        "\n",
        "print(\"Clientes:\")\n",
        "spark.sql(\"SELECT * FROM cliente\").show()\n",
        "\n",
        "print(\"Carros:\")\n",
        "spark.sql(\"SELECT * FROM carro\").show()\n",
        "\n",
        "print(\"Marcas:\")\n",
        "spark.sql(\"SELECT * FROM marca\").show()\n",
        "\n",
        "print(\"Aluguéis:\")\n",
        "spark.sql(\"SELECT * FROM aluguel\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "02 - Exibir as cinco primeiras linhas de cada DataFrame\n"
      ],
      "metadata": {
        "id": "qPFvPFIOQKfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Clientes:\")\n",
        "spark.sql(\"SELECT * FROM cliente\").show(5)\n",
        "\n",
        "print(\"Carros:\")\n",
        "spark.sql(\"SELECT * FROM carro\").show(5)\n",
        "\n",
        "print(\"Marcas:\")\n",
        "spark.sql(\"SELECT * FROM marca\").show(5)\n",
        "\n",
        "print(\"Aluguéis:\")\n",
        "spark.sql(\"SELECT * FROM aluguel\").show(5)\n"
      ],
      "metadata": {
        "id": "-JqPYpEwQFxC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac7e19a-77da-4e6b-f912-32300248fcd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clientes:\n",
            "+----------+----------------+---------------+----+------+-----------+\n",
            "|codcliente|            nome|         cidade|sexo|estado|estadocivil|\n",
            "+----------+----------------+---------------+----+------+-----------+\n",
            "|         1|       Ana Silva|Duque de Caxias|   F|    RJ|          C|\n",
            "|         2|   Bruna Pereira|        Niterói|   F|    RJ|          C|\n",
            "|         3|Túlio Nascimento|Duque de Caxias|   M|    RJ|          S|\n",
            "|         4|  Fernando Souza|       Campinas|   M|    SP|          S|\n",
            "|         5|   Lúcia Andrade|      São Paulo|   F|    SP|          C|\n",
            "+----------+----------------+---------------+----+------+-----------+\n",
            "\n",
            "Carros:\n",
            "+--------+--------+------+-----+\n",
            "|codcarro|codmarca|modelo|valor|\n",
            "+--------+--------+------+-----+\n",
            "|       1|       1|    Ka|100.0|\n",
            "|       2|       2|  Argo|150.0|\n",
            "|       3|       3|  Onix|170.0|\n",
            "|       4|       4|  Polo|150.0|\n",
            "|       5|       5|  Kwid|120.0|\n",
            "+--------+--------+------+-----+\n",
            "\n",
            "Marcas:\n",
            "+--------+----------+\n",
            "|codmarca|     marca|\n",
            "+--------+----------+\n",
            "|       1|      Ford|\n",
            "|       2|      Fiat|\n",
            "|       3| Chevrolet|\n",
            "|       4|Volkswagen|\n",
            "|       5|   Renault|\n",
            "+--------+----------+\n",
            "\n",
            "Aluguéis:\n",
            "+----------+----------+--------+------------+\n",
            "|codaluguel|codcliente|codcarro|data_aluguel|\n",
            "+----------+----------+--------+------------+\n",
            "|         1|         3|       2|  2023-04-01|\n",
            "|         2|         2|       1|  2023-04-02|\n",
            "|         3|         2|       1|  2023-04-03|\n",
            "|         4|         2|       3|  2023-04-04|\n",
            "|         5|         1|       4|  2023-04-05|\n",
            "+----------+----------+--------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "03 - Contar o número de linhas e colunas de cada tabela\n"
      ],
      "metadata": {
        "id": "iATSAM3xQK35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contar_tabela(nome_tabela):\n",
        "    num_linhas = spark.sql(f\"SELECT COUNT(*) FROM {nome_tabela}\").collect()[0][0]\n",
        "    num_colunas = len(spark.sql(f\"DESCRIBE {nome_tabela}\").collect())\n",
        "    print(f\"Tabela '{nome_tabela}': {num_linhas} linhas, {num_colunas} colunas\")\n",
        "\n",
        "contar_tabela(\"cliente\")\n",
        "contar_tabela(\"carro\")\n",
        "contar_tabela(\"marca\")\n",
        "contar_tabela(\"aluguel\")\n"
      ],
      "metadata": {
        "id": "AdHlLsLbQF4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19672e0-2c65-4e05-e162-83595791bba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabela 'cliente': 5 linhas, 6 colunas\n",
            "Tabela 'carro': 5 linhas, 4 colunas\n",
            "Tabela 'marca': 5 linhas, 2 colunas\n",
            "Tabela 'aluguel': 10 linhas, 4 colunas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "04 - Exibir o esquema (schema) de cada DataFrame\n"
      ],
      "metadata": {
        "id": "V8WKnmUhQLTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ExibirEsquema\").getOrCreate()\n",
        "\n",
        "\n",
        "df_clientes = spark.read.option(\"header\", \"true\").csv(\"cliente.csv\")\n",
        "df_carros = spark.read.option(\"header\", \"true\").csv(\"carro.csv\")\n",
        "df_marcas = spark.read.option(\"header\", \"true\").csv(\"marca.csv\")\n",
        "df_alugueis = spark.read.option(\"header\", \"true\").csv(\"aluguel.csv\")\n",
        "\n",
        "\n",
        "print(\"Esquema da tabela Clientes:\")\n",
        "df_clientes.printSchema()\n",
        "\n",
        "print(\"Esquema da tabela Carros:\")\n",
        "df_carros.printSchema()\n",
        "\n",
        "print(\"Esquema da tabela Marcas:\")\n",
        "df_marcas.printSchema()\n",
        "\n",
        "print(\"Esquema da tabela Aluguéis:\")\n",
        "df_alugueis.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "id": "EmcdRPo0QGHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a330b1-b69a-4bce-e3df-817395a36138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esquema da tabela Clientes:\n",
            "root\n",
            " |-- codcliente: string (nullable = true)\n",
            " |-- nome: string (nullable = true)\n",
            " |-- cidade: string (nullable = true)\n",
            " |-- sexo: string (nullable = true)\n",
            " |-- estado: string (nullable = true)\n",
            " |-- estadocivil: string (nullable = true)\n",
            "\n",
            "Esquema da tabela Carros:\n",
            "root\n",
            " |-- codcarro: string (nullable = true)\n",
            " |-- codmarca: string (nullable = true)\n",
            " |-- modelo: string (nullable = true)\n",
            " |-- valor: string (nullable = true)\n",
            "\n",
            "Esquema da tabela Marcas:\n",
            "root\n",
            " |-- codmarca: string (nullable = true)\n",
            " |-- marca: string (nullable = true)\n",
            "\n",
            "Esquema da tabela Aluguéis:\n",
            "root\n",
            " |-- codaluguel: string (nullable = true)\n",
            " |-- codcliente: string (nullable = true)\n",
            " |-- codcarro: string (nullable = true)\n",
            " |-- data_aluguel: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "05 - Renomear as colunas dos DataFrames para ter nomes mais amigáveis\n"
      ],
      "metadata": {
        "id": "KSorc_2qQMRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_clientes = df_clientes \\\n",
        "    .withColumnRenamed(\"codcliente\", \"ID_Cliente\") \\\n",
        "    .withColumnRenamed(\"nome\", \"Nome\") \\\n",
        "    .withColumnRenamed(\"cidade\", \"Cidade\") \\\n",
        "    .withColumnRenamed(\"sexo\", \"Sexo\") \\\n",
        "    .withColumnRenamed(\"estado\", \"Estado\") \\\n",
        "    .withColumnRenamed(\"estadocivil\", \"Estado_Civil\")\n",
        "\n",
        "df_carros = df_carros \\\n",
        "    .withColumnRenamed(\"codcarro\", \"ID_Carro\") \\\n",
        "    .withColumnRenamed(\"codmarca\", \"ID_Marca\") \\\n",
        "    .withColumnRenamed(\"modelo\", \"Modelo\") \\\n",
        "    .withColumnRenamed(\"valor\", \"Valor_Diaria\")\n",
        "\n",
        "df_marcas = df_marcas \\\n",
        "    .withColumnRenamed(\"codmarca\", \"ID_Marca\") \\\n",
        "    .withColumnRenamed(\"marca\", \"Nome_Marca\")\n",
        "\n",
        "df_alugueis = df_alugueis \\\n",
        "    .withColumnRenamed(\"codaluguel\", \"ID_Aluguel\") \\\n",
        "    .withColumnRenamed(\"codcliente\", \"ID_Cliente\") \\\n",
        "    .withColumnRenamed(\"codcarro\", \"ID_Carro\") \\\n",
        "    .withColumnRenamed(\"data_aluguel\", \"Data_Aluguel\")\n",
        "\n",
        "print(\"Esquema atualizado da tabela Clientes:\")\n",
        "df_clientes.printSchema()\n",
        "\n",
        "print(\"\\n Esquema atualizado da tabela Carros:\")\n",
        "df_carros.printSchema()\n",
        "\n",
        "print(\"\\n Esquema atualizado da tabela Marcas:\")\n",
        "df_marcas.printSchema()\n",
        "\n",
        "print(\"\\n Esquema atualizado da tabela Aluguéis:\")\n",
        "df_alugueis.printSchema()\n"
      ],
      "metadata": {
        "id": "4FLaDGlfQGai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe60925f-528d-41f1-a2d7-bfbb463a5dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esquema atualizado da tabela Clientes:\n",
            "root\n",
            " |-- ID_Cliente: string (nullable = true)\n",
            " |-- Nome: string (nullable = true)\n",
            " |-- Cidade: string (nullable = true)\n",
            " |-- Sexo: string (nullable = true)\n",
            " |-- Estado: string (nullable = true)\n",
            " |-- Estado_Civil: string (nullable = true)\n",
            "\n",
            "\n",
            " Esquema atualizado da tabela Carros:\n",
            "root\n",
            " |-- ID_Carro: string (nullable = true)\n",
            " |-- ID_Marca: string (nullable = true)\n",
            " |-- Modelo: string (nullable = true)\n",
            " |-- Valor_Diaria: string (nullable = true)\n",
            "\n",
            "\n",
            " Esquema atualizado da tabela Marcas:\n",
            "root\n",
            " |-- ID_Marca: string (nullable = true)\n",
            " |-- Nome_Marca: string (nullable = true)\n",
            "\n",
            "\n",
            " Esquema atualizado da tabela Aluguéis:\n",
            "root\n",
            " |-- ID_Aluguel: string (nullable = true)\n",
            " |-- ID_Cliente: string (nullable = true)\n",
            " |-- ID_Carro: string (nullable = true)\n",
            " |-- Data_Aluguel: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "06 - Selecionar apenas os aluguéis realizados após uma data específica\n"
      ],
      "metadata": {
        "id": "HeO-q3SRQNMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_corte = '2023-01-01'\n",
        "\n",
        "df_alugueis_filtrados = spark.sql(f\"\"\"\n",
        "    SELECT *\n",
        "    FROM aluguel\n",
        "    WHERE CAST(Data_Aluguel AS DATE) > '{data_corte}'\n",
        "\"\"\")\n",
        "\n",
        "df_alugueis_filtrados.show()\n"
      ],
      "metadata": {
        "id": "Z4IB8mCqQGii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a617e9c-3ea1-4ca2-c0f6-6b429fe82883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+------------+\n",
            "|codaluguel|codcliente|codcarro|data_aluguel|\n",
            "+----------+----------+--------+------------+\n",
            "|         1|         3|       2|  2023-04-01|\n",
            "|         2|         2|       1|  2023-04-02|\n",
            "|         3|         2|       1|  2023-04-03|\n",
            "|         4|         2|       3|  2023-04-04|\n",
            "|         5|         1|       4|  2023-04-05|\n",
            "|         6|         1|       4|  2023-04-13|\n",
            "|         7|         1|       1|  2023-04-15|\n",
            "|         8|         5|       2|  2023-04-19|\n",
            "|         9|         5|       2|  2023-04-21|\n",
            "|        10|         3|       1|  2023-04-25|\n",
            "+----------+----------+--------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "07 - Encontrar clientes que residem no estado de \"RJ\"\n"
      ],
      "metadata": {
        "id": "oWEm1cZwQL0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clientes_rj = spark.sql(\"\"\"\n",
        "    SELECT *\n",
        "    FROM cliente\n",
        "    WHERE Estado = 'RJ'\n",
        "\"\"\")\n",
        "\n",
        "df_clientes_rj.show()\n"
      ],
      "metadata": {
        "id": "GmW8bJgwQGp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbfb2de9-5a9e-46e3-8a0e-b0a924050474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------+---------------+----+------+-----------+\n",
            "|codcliente|            nome|         cidade|sexo|estado|estadocivil|\n",
            "+----------+----------------+---------------+----+------+-----------+\n",
            "|         1|       Ana Silva|Duque de Caxias|   F|    RJ|          C|\n",
            "|         2|   Bruna Pereira|        Niterói|   F|    RJ|          C|\n",
            "|         3|Túlio Nascimento|Duque de Caxias|   M|    RJ|          S|\n",
            "+----------+----------------+---------------+----+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "08 - Filtrar carros com valor de aluguel maior que 150"
      ],
      "metadata": {
        "id": "29kvpjZfQMtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_carros_filtrados = spark.sql(\"\"\"\n",
        "    SELECT *\n",
        "    FROM carro\n",
        "    WHERE valor > 150\n",
        "\"\"\")\n",
        "\n",
        "df_carros_filtrados.show()\n"
      ],
      "metadata": {
        "id": "8q-ochDSQGw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c60821-d19f-4d98-c988-3aaf1f5c8be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+------+-----+\n",
            "|codcarro|codmarca|modelo|valor|\n",
            "+--------+--------+------+-----+\n",
            "|       3|       3|  Onix|170.0|\n",
            "+--------+--------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "09 - Selecionar aluguéis onde o cliente é do sexo feminino\n"
      ],
      "metadata": {
        "id": "Wy3IrniBQNrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_alugueis_mulheres = spark.sql(\"\"\"\n",
        "    SELECT a.*\n",
        "    FROM aluguel a\n",
        "    INNER JOIN cliente c ON a.codcliente = c.codcliente\n",
        "    WHERE c.sexo = 'F'\n",
        "\"\"\")\n",
        "\n",
        "df_alugueis_mulheres.show()\n"
      ],
      "metadata": {
        "id": "F_so1s8uQG4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "903060d5-1d2e-4bed-97b9-c7437372acd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+------------+\n",
            "|codaluguel|codcliente|codcarro|data_aluguel|\n",
            "+----------+----------+--------+------------+\n",
            "|         2|         2|       1|  2023-04-02|\n",
            "|         3|         2|       1|  2023-04-03|\n",
            "|         4|         2|       3|  2023-04-04|\n",
            "|         5|         1|       4|  2023-04-05|\n",
            "|         6|         1|       4|  2023-04-13|\n",
            "|         7|         1|       1|  2023-04-15|\n",
            "|         8|         5|       2|  2023-04-19|\n",
            "|         9|         5|       2|  2023-04-21|\n",
            "+----------+----------+--------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 - Identificar clientes solteiros\n"
      ],
      "metadata": {
        "id": "7eSduwTPQOKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clientes_solteiros = spark.sql(\"\"\"\n",
        "    SELECT * FROM cliente\n",
        "    WHERE estadocivil = 'S'\n",
        "\"\"\")\n",
        "\n",
        "df_clientes_solteiros.show()\n"
      ],
      "metadata": {
        "id": "B4zCfqavQG_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53369421-c9a1-4c82-cd81-92fb98181011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------+---------------+----+------+-----------+\n",
            "|codcliente|            nome|         cidade|sexo|estado|estadocivil|\n",
            "+----------+----------------+---------------+----+------+-----------+\n",
            "|         3|Túlio Nascimento|Duque de Caxias|   M|    RJ|          S|\n",
            "|         4|  Fernando Souza|       Campinas|   M|    SP|          S|\n",
            "+----------+----------------+---------------+----+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11 - Realizar um join entre \"Aluguel\" e \"Cliente\" para adicionar informações do cliente ao DataFrame de aluguéis\n"
      ],
      "metadata": {
        "id": "BfhrIo1PQPEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_aluguel_cliente = spark.sql(\"\"\"\n",
        "    SELECT a.*, c.nome, c.cidade, c.estado, c.sexo, c.estadocivil\n",
        "    FROM aluguel a\n",
        "    INNER JOIN cliente c ON a.codcliente = c.codcliente\n",
        "\"\"\")\n",
        "\n",
        "df_aluguel_cliente.show()\n"
      ],
      "metadata": {
        "id": "8VkcuiHrQHGQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e64978b9-8d69-447f-b582-0be074b432d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+------------+----------------+---------------+------+----+-----------+\n",
            "|codaluguel|codcliente|codcarro|data_aluguel|            nome|         cidade|estado|sexo|estadocivil|\n",
            "+----------+----------+--------+------------+----------------+---------------+------+----+-----------+\n",
            "|         7|         1|       1|  2023-04-15|       Ana Silva|Duque de Caxias|    RJ|   F|          C|\n",
            "|         6|         1|       4|  2023-04-13|       Ana Silva|Duque de Caxias|    RJ|   F|          C|\n",
            "|         5|         1|       4|  2023-04-05|       Ana Silva|Duque de Caxias|    RJ|   F|          C|\n",
            "|         4|         2|       3|  2023-04-04|   Bruna Pereira|        Niterói|    RJ|   F|          C|\n",
            "|         3|         2|       1|  2023-04-03|   Bruna Pereira|        Niterói|    RJ|   F|          C|\n",
            "|         2|         2|       1|  2023-04-02|   Bruna Pereira|        Niterói|    RJ|   F|          C|\n",
            "|        10|         3|       1|  2023-04-25|Túlio Nascimento|Duque de Caxias|    RJ|   M|          S|\n",
            "|         1|         3|       2|  2023-04-01|Túlio Nascimento|Duque de Caxias|    RJ|   M|          S|\n",
            "|         9|         5|       2|  2023-04-21|   Lúcia Andrade|      São Paulo|    SP|   F|          C|\n",
            "|         8|         5|       2|  2023-04-19|   Lúcia Andrade|      São Paulo|    SP|   F|          C|\n",
            "+----------+----------+--------+------------+----------------+---------------+------+----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12 - Juntar \"Carro\" e \"Marca\" para incluir o nome da marca no DataFrame de carros\n"
      ],
      "metadata": {
        "id": "1AUOUzcSQPn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_carro_marca = spark.sql(\"\"\"\n",
        "    SELECT c.*, m.marca\n",
        "    FROM carro c\n",
        "    INNER JOIN marca m ON c.codmarca = m.codmarca\n",
        "\"\"\")\n",
        "\n",
        "df_carro_marca.show()\n"
      ],
      "metadata": {
        "id": "quKZJK11QHNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5851cabe-9087-4e33-eb1f-3f3edea68366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+------+-----+----------+\n",
            "|codcarro|codmarca|modelo|valor|     marca|\n",
            "+--------+--------+------+-----+----------+\n",
            "|       1|       1|    Ka|100.0|      Ford|\n",
            "|       2|       2|  Argo|150.0|      Fiat|\n",
            "|       3|       3|  Onix|170.0| Chevrolet|\n",
            "|       4|       4|  Polo|150.0|Volkswagen|\n",
            "|       5|       5|  Kwid|120.0|   Renault|\n",
            "+--------+--------+------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "13 - Criar um DataFrame combinando \"Aluguel\", \"Carro\" e \"Cliente\"\n"
      ],
      "metadata": {
        "id": "U3jMvQlSQP9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_aluguel_carro_cliente = spark.sql(\"\"\"\n",
        "    SELECT a.*, c.nome, c.cidade, c.estado, c.sexo, c.estadocivil,\n",
        "           car.modelo, car.valor\n",
        "    FROM aluguel a\n",
        "    INNER JOIN cliente c ON a.codcliente = c.codcliente\n",
        "    INNER JOIN carro car ON a.codcarro = car.codcarro\n",
        "\"\"\")\n",
        "\n",
        "df_aluguel_carro_cliente.show()\n"
      ],
      "metadata": {
        "id": "1pY0zV0hQHUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41daa39b-142d-48f8-9572-a6b9203674f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+------------+----------------+---------------+------+----+-----------+------+-----+\n",
            "|codaluguel|codcliente|codcarro|data_aluguel|            nome|         cidade|estado|sexo|estadocivil|modelo|valor|\n",
            "+----------+----------+--------+------------+----------------+---------------+------+----+-----------+------+-----+\n",
            "|         7|         1|       1|  2023-04-15|       Ana Silva|Duque de Caxias|    RJ|   F|          C|    Ka|100.0|\n",
            "|         6|         1|       4|  2023-04-13|       Ana Silva|Duque de Caxias|    RJ|   F|          C|  Polo|150.0|\n",
            "|         5|         1|       4|  2023-04-05|       Ana Silva|Duque de Caxias|    RJ|   F|          C|  Polo|150.0|\n",
            "|         4|         2|       3|  2023-04-04|   Bruna Pereira|        Niterói|    RJ|   F|          C|  Onix|170.0|\n",
            "|         3|         2|       1|  2023-04-03|   Bruna Pereira|        Niterói|    RJ|   F|          C|    Ka|100.0|\n",
            "|         2|         2|       1|  2023-04-02|   Bruna Pereira|        Niterói|    RJ|   F|          C|    Ka|100.0|\n",
            "|        10|         3|       1|  2023-04-25|Túlio Nascimento|Duque de Caxias|    RJ|   M|          S|    Ka|100.0|\n",
            "|         1|         3|       2|  2023-04-01|Túlio Nascimento|Duque de Caxias|    RJ|   M|          S|  Argo|150.0|\n",
            "|         9|         5|       2|  2023-04-21|   Lúcia Andrade|      São Paulo|    SP|   F|          C|  Argo|150.0|\n",
            "|         8|         5|       2|  2023-04-19|   Lúcia Andrade|      São Paulo|    SP|   F|          C|  Argo|150.0|\n",
            "+----------+----------+--------+------------+----------------+---------------+------+----+-----------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14 - Realizar um join entre \"Cliente\" e \"Carro\" com uma condição específica"
      ],
      "metadata": {
        "id": "PSVsV4JLQQhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cliente_carro_filtrado = spark.sql(\"\"\"\n",
        "    SELECT c.*, car.modelo, car.valor\n",
        "    FROM cliente c\n",
        "    INNER JOIN aluguel a ON c.codcliente = a.codcliente\n",
        "    INNER JOIN carro car ON a.codcarro = car.codcarro\n",
        "    WHERE car.valor > 100\n",
        "\"\"\")\n",
        "\n",
        "df_cliente_carro_filtrado.show()\n"
      ],
      "metadata": {
        "id": "Fv8cJn-uQHbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4627b7c3-bc6d-4a45-8855-51eae48a5b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------+---------------+----+------+-----------+------+-----+\n",
            "|codcliente|            nome|         cidade|sexo|estado|estadocivil|modelo|valor|\n",
            "+----------+----------------+---------------+----+------+-----------+------+-----+\n",
            "|         1|       Ana Silva|Duque de Caxias|   F|    RJ|          C|  Polo|150.0|\n",
            "|         1|       Ana Silva|Duque de Caxias|   F|    RJ|          C|  Polo|150.0|\n",
            "|         2|   Bruna Pereira|        Niterói|   F|    RJ|          C|  Onix|170.0|\n",
            "|         3|Túlio Nascimento|Duque de Caxias|   M|    RJ|          S|  Argo|150.0|\n",
            "|         5|   Lúcia Andrade|      São Paulo|   F|    SP|          C|  Argo|150.0|\n",
            "|         5|   Lúcia Andrade|      São Paulo|   F|    SP|          C|  Argo|150.0|\n",
            "+----------+----------------+---------------+----+------+-----------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15 - Encontrar o valor médio dos carros alugados\n"
      ],
      "metadata": {
        "id": "1wqaulyOQQ93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_valor_medio_carros = spark.sql(\"\"\"\n",
        "    SELECT AVG(car.valor) AS valor_medio\n",
        "    FROM aluguel a\n",
        "    INNER JOIN carro car ON a.codcarro = car.codcarro\n",
        "\"\"\")\n",
        "\n",
        "df_valor_medio_carros.show()\n"
      ],
      "metadata": {
        "id": "ol10uZI2QHh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df241581-55ae-4e72-88ff-8091e5fd9abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|valor_medio|\n",
            "+-----------+\n",
            "|      132.0|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16 - Calcular o número total de clientes por estado\n"
      ],
      "metadata": {
        "id": "kC-om0u9QRWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clientes_por_estado = spark.sql(\"\"\"\n",
        "    SELECT estado, COUNT(*) AS total_clientes\n",
        "    FROM cliente\n",
        "    GROUP BY estado\n",
        "\"\"\")\n",
        "\n",
        "df_clientes_por_estado.show()\n"
      ],
      "metadata": {
        "id": "1fCrRatNQHos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e48b22ca-a2a7-448c-a1ea-bdf2bd4cfd58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+\n",
            "|estado|total_clientes|\n",
            "+------+--------------+\n",
            "|    SP|             2|\n",
            "|    RJ|             3|\n",
            "+------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17 - Identificar a marca mais popular com base nos aluguéis"
      ],
      "metadata": {
        "id": "jz-UaWttQR8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_marca_popular = spark.sql(\"\"\"\n",
        "    SELECT m.marca, COUNT(*) AS total_alugueis\n",
        "    FROM aluguel a\n",
        "    INNER JOIN carro c ON a.codcarro = c.codcarro\n",
        "    INNER JOIN marca m ON c.codmarca = m.codmarca\n",
        "    GROUP BY m.marca\n",
        "    ORDER BY total_alugueis DESC\n",
        "    LIMIT 1\n",
        "\"\"\")\n",
        "\n",
        "df_marca_popular.show()\n"
      ],
      "metadata": {
        "id": "fCCTn1BAQHvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4beaedb4-2533-45d4-c86f-b93ed53a22af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------+\n",
            "|marca|total_alugueis|\n",
            "+-----+--------------+\n",
            "| Ford|             4|\n",
            "+-----+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18 - Determinar o maior e menor valor de aluguel entre os carros\n"
      ],
      "metadata": {
        "id": "803M8LbhQSQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_valores_aluguel = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        MAX(valor) AS valor_maximo,\n",
        "        MIN(valor) AS valor_minimo\n",
        "    FROM carro\n",
        "\"\"\")\n",
        "\n",
        "df_valores_aluguel.show()\n"
      ],
      "metadata": {
        "id": "mxKRsD9JQH17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddbc9c91-72b9-41fc-f7e0-b3b25dd13feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+\n",
            "|valor_maximo|valor_minimo|\n",
            "+------------+------------+\n",
            "|       170.0|       100.0|\n",
            "+------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19 - Classificar os carros pelo valor do aluguel em ordem decrescente\n"
      ],
      "metadata": {
        "id": "bY4nat8GRK0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_carros_ordenados = spark.sql(\"\"\"\n",
        "    SELECT *\n",
        "    FROM carro\n",
        "    ORDER BY valor DESC\n",
        "\"\"\")\n",
        "\n",
        "df_carros_ordenados.show()\n"
      ],
      "metadata": {
        "id": "QEPRcJDyQH8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fbef4c0-6293-4ebb-8f7f-92ab84b01aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+------+-----+\n",
            "|codcarro|codmarca|modelo|valor|\n",
            "+--------+--------+------+-----+\n",
            "|       3|       3|  Onix|170.0|\n",
            "|       4|       4|  Polo|150.0|\n",
            "|       2|       2|  Argo|150.0|\n",
            "|       5|       5|  Kwid|120.0|\n",
            "|       1|       1|    Ka|100.0|\n",
            "+--------+--------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20 - Calcular a diferença em dias entre o aluguel mais recente e o mais antigo\n"
      ],
      "metadata": {
        "id": "D5nUNDtyQTAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_diferenca = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        MAX(data_aluguel) AS data_recente,\n",
        "        MIN(data_aluguel) AS data_antiga,\n",
        "        DATEDIFF(MAX(data_aluguel), MIN(data_aluguel)) AS diferenca_dias\n",
        "    FROM aluguel\n",
        "\"\"\")\n",
        "\n",
        "df_diferenca.show()\n"
      ],
      "metadata": {
        "id": "h5QABOJLRJjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d31ddd-1b94-4327-fc2b-ce0eac6d8105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+--------------+\n",
            "|data_recente|data_antiga|diferenca_dias|\n",
            "+------------+-----------+--------------+\n",
            "|  2023-04-25| 2023-04-01|            24|\n",
            "+------------+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21 - Criar uma coluna no DataFrame \"Carro\" para categorizar os valores de aluguel\n"
      ],
      "metadata": {
        "id": "z-xBmC9oQSrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_carros_categorizados = spark.sql(\"\"\"\n",
        "    SELECT *,\n",
        "        CASE\n",
        "            WHEN valor <= 100 THEN 'Econômico'\n",
        "            WHEN valor > 100 AND valor <= 200 THEN 'Intermediário'\n",
        "            ELSE 'Luxo'\n",
        "        END AS categoria_preco\n",
        "    FROM carro\n",
        "\"\"\")\n",
        "\n",
        "df_carros_categorizados.show()\n"
      ],
      "metadata": {
        "id": "PpQkkLo1QGAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7bc3eb-68ac-4b8e-bfa3-1695037cfd9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+------+-----+---------------+\n",
            "|codcarro|codmarca|modelo|valor|categoria_preco|\n",
            "+--------+--------+------+-----+---------------+\n",
            "|       1|       1|    Ka|100.0|      Econômico|\n",
            "|       2|       2|  Argo|150.0|  Intermediário|\n",
            "|       3|       3|  Onix|170.0|  Intermediário|\n",
            "|       4|       4|  Polo|150.0|  Intermediário|\n",
            "|       5|       5|  Kwid|120.0|  Intermediário|\n",
            "+--------+--------+------+-----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22 - Criar uma coluna no DataFrame \"Cliente\" para indicar se a cidade é a capital do estado\n"
      ],
      "metadata": {
        "id": "ZZ4S4xhFRRmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clientes_capital = spark.sql(\"\"\"\n",
        "    SELECT *,\n",
        "        CASE\n",
        "            WHEN (Estado = 'AC' AND cidade = 'Rio Branco') OR\n",
        "                 (Estado = 'AL' AND cidade = 'Maceió') OR\n",
        "                 (Estado = 'AP' AND cidade = 'Macapá') OR\n",
        "                 (Estado = 'AM' AND cidade = 'Manaus') OR\n",
        "                 (Estado = 'BA' AND cidade = 'Salvador') OR\n",
        "                 (Estado = 'CE' AND cidade = 'Fortaleza') OR\n",
        "                 (Estado = 'DF' AND cidade = 'Brasília') OR\n",
        "                 (Estado = 'ES' AND cidade = 'Vitória') OR\n",
        "                 (Estado = 'GO' AND cidade = 'Goiânia') OR\n",
        "                 (Estado = 'MA' AND cidade = 'São Luís') OR\n",
        "                 (Estado = 'MT' AND cidade = 'Cuiabá') OR\n",
        "                 (Estado = 'MS' AND cidade = 'Campo Grande') OR\n",
        "                 (Estado = 'MG' AND cidade = 'Belo Horizonte') OR\n",
        "                 (Estado = 'PA' AND cidade = 'Belém') OR\n",
        "                 (Estado = 'PB' AND cidade = 'João Pessoa') OR\n",
        "                 (Estado = 'PR' AND cidade = 'Curitiba') OR\n",
        "                 (Estado = 'PE' AND cidade = 'Recife') OR\n",
        "                 (Estado = 'PI' AND cidade = 'Teresina') OR\n",
        "                 (Estado = 'RJ' AND cidade = 'Rio de Janeiro') OR\n",
        "                 (Estado = 'RN' AND cidade = 'Natal') OR\n",
        "                 (Estado = 'RS' AND cidade = 'Porto Alegre') OR\n",
        "                 (Estado = 'RO' AND cidade = 'Porto Velho') OR\n",
        "                 (Estado = 'RR' AND cidade = 'Boa Vista') OR\n",
        "                 (Estado = 'SC' AND cidade = 'Florianópolis') OR\n",
        "                 (Estado = 'SP' AND cidade = 'São Paulo') OR\n",
        "                 (Estado = 'SE' AND cidade = 'Aracaju') OR\n",
        "                 (Estado = 'TO' AND cidade = 'Palmas')\n",
        "            THEN 'Sim'\n",
        "            ELSE 'Não'\n",
        "        END AS e_capital\n",
        "    FROM cliente\n",
        "\"\"\")\n",
        "\n",
        "df_clientes_capital.show()\n"
      ],
      "metadata": {
        "id": "PLjvdGNERR86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac364e99-b3c3-4637-8afe-38f592c1d411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------+---------------+----+------+-----------+---------+\n",
            "|codcliente|            nome|         cidade|sexo|estado|estadocivil|e_capital|\n",
            "+----------+----------------+---------------+----+------+-----------+---------+\n",
            "|         1|       Ana Silva|Duque de Caxias|   F|    RJ|          C|      Não|\n",
            "|         2|   Bruna Pereira|        Niterói|   F|    RJ|          C|      Não|\n",
            "|         3|Túlio Nascimento|Duque de Caxias|   M|    RJ|          S|      Não|\n",
            "|         4|  Fernando Souza|       Campinas|   M|    SP|          S|      Não|\n",
            "|         5|   Lúcia Andrade|      São Paulo|   F|    SP|          C|      Sim|\n",
            "+----------+----------------+---------------+----+------+-----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23 - Adicionar uma coluna em \"Aluguel\" com o valor total do aluguel, considerando uma taxa fixa de 10%"
      ],
      "metadata": {
        "id": "6Kdi9CxuRSLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_alugueis.createOrReplaceTempView(\"aluguel\")\n",
        "df_carros.createOrReplaceTempView(\"carro\")\n",
        "\n",
        "\n",
        "df_alugueis_com_valor_total = spark.sql(\"\"\"\n",
        "    SELECT a.*,\n",
        "           c.Valor_Diaria * 1.10 AS valor_total\n",
        "    FROM aluguel a\n",
        "    INNER JOIN carro c ON a.ID_Carro = c.ID_Carro\n",
        "\"\"\")\n",
        "\n",
        "df_alugueis_com_valor_total.show()\n"
      ],
      "metadata": {
        "id": "DYknl66TRShM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e1eb1b3-b211-4444-890f-f9fe9b92c619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+------------+------------------+\n",
            "|ID_Aluguel|ID_Cliente|ID_Carro|Data_Aluguel|       valor_total|\n",
            "+----------+----------+--------+------------+------------------+\n",
            "|         1|         3|       2|  2023-04-01|             165.0|\n",
            "|         2|         2|       1|  2023-04-02|110.00000000000001|\n",
            "|         3|         2|       1|  2023-04-03|110.00000000000001|\n",
            "|         4|         2|       3|  2023-04-04|187.00000000000003|\n",
            "|         5|         1|       4|  2023-04-05|             165.0|\n",
            "|         6|         1|       4|  2023-04-13|             165.0|\n",
            "|         7|         1|       1|  2023-04-15|110.00000000000001|\n",
            "|         8|         5|       2|  2023-04-19|             165.0|\n",
            "|         9|         5|       2|  2023-04-21|             165.0|\n",
            "|        10|         3|       1|  2023-04-25|110.00000000000001|\n",
            "+----------+----------+--------+------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24 - Agrupar os aluguéis por cliente e contar o número de carros alugados"
      ],
      "metadata": {
        "id": "qhyY0QNsRW03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_alugueis_por_cliente = spark.sql(\"\"\"\n",
        "    SELECT codcliente, COUNT(codcarro) AS num_carros_alugados\n",
        "    FROM aluguel\n",
        "    GROUP BY codcliente\n",
        "\"\"\")\n",
        "\n",
        "df_alugueis_por_cliente.show()\n"
      ],
      "metadata": {
        "id": "ytiddFRyRXFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2951fe61-1a85-490f-bae1-b09e582e6c79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+\n",
            "|codcliente|num_carros_alugados|\n",
            "+----------+-------------------+\n",
            "|         1|                  3|\n",
            "|         3|                  2|\n",
            "|         5|                  2|\n",
            "|         2|                  3|\n",
            "+----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25 - Criar um script PySpark para agendar a execução automática das transformações\n"
      ],
      "metadata": {
        "id": "O_NNDlzGRXTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apache-airflow\n",
        "\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "from datetime import datetime\n",
        "\n",
        "# Função que será chamada para realizar as transformações no PySpark\n",
        "def executar_transformacoes_pyspark():\n",
        "    # Iniciar uma sessão Spark\n",
        "    spark = SparkSession.builder.appName(\"TransformacoesLocadora\").getOrCreate()\n",
        "\n",
        "    # Leitura dos dados\n",
        "    df_carros = spark.read.option(\"header\", \"true\").csv(\"carros.csv\")\n",
        "    df_clientes = spark.read.option(\"header\", \"true\").csv(\"clientes.csv\")\n",
        "    df_alugueis = spark.read.option(\"header\", \"true\").csv(\"alugueis.csv\")\n",
        "\n",
        "    # Realizar as transformações nos dados\n",
        "    # Exemplo de transformação: Categorizar os carros por preço\n",
        "    df_carros = df_carros.withColumn(\n",
        "        \"categoria_preco\",\n",
        "        when(col(\"valor\") <= 100, \"Econômico\")\n",
        "        .when((col(\"valor\") > 100) & (col(\"valor\") <= 200), \"Intermediário\")\n",
        "        .otherwise(\"Luxo\")\n",
        "    )\n",
        "\n",
        "    # Exemplo de transformação: Filtrar aluguéis realizados após uma data específica\n",
        "    data_corte = \"2025-01-01\"\n",
        "    df_alugueis_filtrados = df_alugueis.filter(col(\"data_aluguel\") > data_corte)\n",
        "\n",
        "    # Salvar os resultados transformados em arquivos CSV\n",
        "    df_carros.write.mode(\"overwrite\").csv(\"carros_transformados.csv\")\n",
        "    df_alugueis_filtrados.write.mode(\"overwrite\").csv(\"alugueis_filtrados.csv\")\n",
        "\n",
        "    # Fechar a sessão Spark\n",
        "    spark.stop()\n",
        "\n",
        "# Definir o DAG (Directed Acyclic Graph)\n",
        "dag = DAG(\n",
        "    \"transformacoes_locadora\",  # Nome do DAG\n",
        "    description=\"Agendamento de transformações no PySpark\",  # Descrição do DAG\n",
        "    schedule_interval=\"0 * * * *\",  # Agendar para rodar a cada hora\n",
        "    start_date=datetime(2025, 4, 1),  # Data de início das execuções\n",
        "    catchup=False,  # Evitar execução retroativa\n",
        ")\n",
        "\n",
        "# Definir a tarefa dentro do DAG\n",
        "tarefa_transformacoes = PythonOperator(\n",
        "    task_id=\"executar_transformacoes_pyspark\",  # Nome da tarefa\n",
        "    python_callable=executar_transformacoes_pyspark,  # Função a ser chamada\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Rodar a tarefa\n",
        "tarefa_transformacoes\n"
      ],
      "metadata": {
        "id": "5nXhkNwORXmq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "149af768-9480-4c34-b12f-23435d20f1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: apache-airflow in /usr/local/lib/python3.11/dist-packages (2.10.5)\n",
            "Requirement already satisfied: alembic<2.0,>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.15.2)\n",
            "Requirement already satisfied: argcomplete>=1.10 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (3.6.1)\n",
            "Requirement already satisfied: asgiref>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (3.8.1)\n",
            "Requirement already satisfied: attrs>=22.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (25.3.0)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.9.0)\n",
            "Requirement already satisfied: colorlog>=6.8.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (6.9.0)\n",
            "Requirement already satisfied: configupdater>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (3.2)\n",
            "Requirement already satisfied: connexion<3.0,>=2.14.2 in /usr/local/lib/python3.11/dist-packages (from connexion[flask]<3.0,>=2.14.2->apache-airflow) (2.14.2)\n",
            "Requirement already satisfied: cron-descriptor>=1.2.24 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.4.5)\n",
            "Requirement already satisfied: croniter>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (6.0.0)\n",
            "Requirement already satisfied: cryptography>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (43.0.3)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.2.18)\n",
            "Requirement already satisfied: dill>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (0.3.9)\n",
            "Requirement already satisfied: flask-caching>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (2.3.1)\n",
            "Requirement already satisfied: flask-session<0.6,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (0.5.0)\n",
            "Requirement already satisfied: flask-wtf>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.2.2)\n",
            "Requirement already satisfied: flask<2.3,>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (2.2.5)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (2025.3.0)\n",
            "Requirement already satisfied: google-re2>=1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.1.20240702)\n",
            "Requirement already satisfied: gunicorn>=20.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (23.0.0)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (0.28.1)\n",
            "Requirement already satisfied: importlib_metadata>=6.5 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (8.6.1)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (4.23.0)\n",
            "Requirement already satisfied: lazy-object-proxy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.10.0)\n",
            "Requirement already satisfied: linkify-it-py>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (2.0.3)\n",
            "Requirement already satisfied: lockfile>=0.12.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (0.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (3.0.0)\n",
            "Requirement already satisfied: markupsafe>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (3.0.2)\n",
            "Requirement already satisfied: marshmallow-oneofschema>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (3.1.1)\n",
            "Requirement already satisfied: mdit-py-plugins>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (0.4.2)\n",
            "Requirement already satisfied: methodtools>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (0.4.7)\n",
            "Requirement already satisfied: opentelemetry-api>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.31.1)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (24.2)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (0.12.1)\n",
            "Requirement already satisfied: pendulum<4.0,>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (3.0.0)\n",
            "Requirement already satisfied: pluggy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.5.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (5.9.5)\n",
            "Requirement already satisfied: pygments>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (2.18.0)\n",
            "Requirement already satisfied: pyjwt>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (2.10.1)\n",
            "Requirement already satisfied: python-daemon>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (2.8.2)\n",
            "Requirement already satisfied: python-nvd3>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (0.16.0)\n",
            "Requirement already satisfied: python-slugify>=5.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (8.0.4)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.0.0)\n",
            "Requirement already satisfied: rfc3339-validator>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (0.1.4)\n",
            "Requirement already satisfied: rich-argparse>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.7.0)\n",
            "Requirement already satisfied: rich>=12.4.4 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (13.9.4)\n",
            "Requirement already satisfied: setproctitle>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.3.5)\n",
            "Requirement already satisfied: sqlalchemy<2.0,>=1.4.36 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.4.54)\n",
            "Requirement already satisfied: sqlalchemy-jsonfield>=1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.0.2)\n",
            "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (0.9.0)\n",
            "Requirement already satisfied: tenacity!=8.2.0,>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (9.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (2.5.0)\n",
            "Requirement already satisfied: universal-pathlib!=0.2.4,>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (0.2.6)\n",
            "Requirement already satisfied: werkzeug<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (2.2.3)\n",
            "Requirement already satisfied: apache-airflow-providers-common-compat in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.6.0)\n",
            "Requirement already satisfied: apache-airflow-providers-common-io in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.5.2)\n",
            "Requirement already satisfied: apache-airflow-providers-common-sql in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.24.1)\n",
            "Requirement already satisfied: apache-airflow-providers-fab>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (1.5.3)\n",
            "Requirement already satisfied: apache-airflow-providers-ftp in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (3.12.3)\n",
            "Requirement already satisfied: apache-airflow-providers-http in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (5.2.1)\n",
            "Requirement already satisfied: apache-airflow-providers-imap in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (3.8.3)\n",
            "Requirement already satisfied: apache-airflow-providers-smtp in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (2.0.2)\n",
            "Requirement already satisfied: apache-airflow-providers-sqlite in /usr/local/lib/python3.11/dist-packages (from apache-airflow) (4.0.1)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic<2.0,>=1.13.1->apache-airflow) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic<2.0,>=1.13.1->apache-airflow) (4.13.0)\n",
            "Requirement already satisfied: flask-appbuilder==4.5.3 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-fab>=1.0.2->apache-airflow) (4.5.3)\n",
            "Requirement already satisfied: flask-login>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.6.3)\n",
            "Requirement already satisfied: jmespath>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-fab>=1.0.2->apache-airflow) (1.0.1)\n",
            "Requirement already satisfied: apispec<7,>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from apispec[yaml]<7,>=6.0.0->flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (6.8.1)\n",
            "Requirement already satisfied: colorama<1,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.4.6)\n",
            "Requirement already satisfied: click<9,>=8 in /usr/local/lib/python3.11/dist-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (8.1.8)\n",
            "Requirement already satisfied: email-validator>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.2.0)\n",
            "Requirement already satisfied: Flask-Babel<3,>=1 in /usr/local/lib/python3.11/dist-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.0.0)\n",
            "Requirement already satisfied: Flask-Limiter<4,>3 in /usr/local/lib/python3.11/dist-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (3.12)\n",
            "Requirement already satisfied: Flask-SQLAlchemy<3,>=2.4 in /usr/local/lib/python3.11/dist-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.5.1)\n",
            "Requirement already satisfied: Flask-JWT-Extended<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (4.7.1)\n",
            "Requirement already satisfied: marshmallow<4,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (3.26.1)\n",
            "Requirement already satisfied: marshmallow-sqlalchemy<0.29.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.28.2)\n",
            "Requirement already satisfied: prison<1.0.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.2.1)\n",
            "Requirement already satisfied: sqlalchemy-utils<1,>=0.32.21 in /usr/local/lib/python3.11/dist-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.41.2)\n",
            "Requirement already satisfied: WTForms<4 in /usr/local/lib/python3.11/dist-packages (from flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (3.2.1)\n",
            "Requirement already satisfied: clickclick<21,>=1.2 in /usr/local/lib/python3.11/dist-packages (from connexion<3.0,>=2.14.2->connexion[flask]<3.0,>=2.14.2->apache-airflow) (20.10.2)\n",
            "Requirement already satisfied: PyYAML<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from connexion<3.0,>=2.14.2->connexion[flask]<3.0,>=2.14.2->apache-airflow) (6.0.2)\n",
            "Requirement already satisfied: inflection<0.6,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from connexion<3.0,>=2.14.2->connexion[flask]<3.0,>=2.14.2->apache-airflow) (0.5.1)\n",
            "Requirement already satisfied: pytz>2021.1 in /usr/local/lib/python3.11/dist-packages (from croniter>=2.0.2->apache-airflow) (2025.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=41.0.0->apache-airflow) (1.17.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.13->apache-airflow) (1.17.2)\n",
            "Requirement already satisfied: cachelib>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from flask-caching>=2.0.0->apache-airflow) (0.13.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->apache-airflow) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->apache-airflow) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=6.5->apache-airflow) (3.21.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->apache-airflow) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->apache-airflow) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->apache-airflow) (0.24.0)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py>=2.0.0->apache-airflow) (1.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.1.0->apache-airflow) (0.1.2)\n",
            "Requirement already satisfied: wirerope>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from methodtools>=0.4.7->apache-airflow) (1.0.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (1.31.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.31.1->opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (1.69.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.31.1->opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (1.71.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.31.1->opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.31.1->opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-sdk~=1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.31.1->opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (1.31.1)\n",
            "Requirement already satisfied: protobuf<6.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-proto==1.31.1->opentelemetry-exporter-otlp-proto-grpc==1.31.1->opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (5.29.4)\n",
            "Requirement already satisfied: tzdata>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pendulum<4.0,>=2.1.2->apache-airflow) (2025.2)\n",
            "Requirement already satisfied: time-machine>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from pendulum<4.0,>=2.1.2->apache-airflow) (2.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->apache-airflow) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify>=5.0->apache-airflow) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->apache-airflow) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->apache-airflow) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2.0,>=1.4.36->apache-airflow) (3.1.1)\n",
            "Requirement already satisfied: sqlparse>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-common-sql->apache-airflow) (0.5.3)\n",
            "Requirement already satisfied: more-itertools>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-common-sql->apache-airflow) (10.6.0)\n",
            "Requirement already satisfied: aiohttp!=3.11.0,>=3.9.2 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-http->apache-airflow) (3.11.14)\n",
            "Requirement already satisfied: aiosqlite>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from apache-airflow-providers-sqlite->apache-airflow) (0.21.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=3.11.0,>=3.9.2->apache-airflow-providers-http->apache-airflow) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=3.11.0,>=3.9.2->apache-airflow-providers-http->apache-airflow) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=3.11.0,>=3.9.2->apache-airflow-providers-http->apache-airflow) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=3.11.0,>=3.9.2->apache-airflow-providers-http->apache-airflow) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=3.11.0,>=3.9.2->apache-airflow-providers-http->apache-airflow) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=3.11.0,>=3.9.2->apache-airflow-providers-http->apache-airflow) (1.18.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=41.0.0->apache-airflow) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->apache-airflow) (1.3.1)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=1.0.5->flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.7.0)\n",
            "Requirement already satisfied: Babel>=2.3 in /usr/local/lib/python3.11/dist-packages (from Flask-Babel<3,>=1->flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.17.0)\n",
            "Requirement already satisfied: limits>=3.13 in /usr/local/lib/python3.11/dist-packages (from Flask-Limiter<4,>3->flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (4.4.1)\n",
            "Requirement already satisfied: ordered-set<5,>4 in /usr/local/lib/python3.11/dist-packages (from Flask-Limiter<4,>3->flask-appbuilder==4.5.3->apache-airflow-providers-fab>=1.0.2->apache-airflow) (4.1.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk~=1.31.1->opentelemetry-exporter-otlp-proto-grpc==1.31.1->opentelemetry-exporter-otlp>=1.24.0->apache-airflow) (0.52b1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m<\u001b[0m\u001b[1;33mipython-input-\u001b[0m\u001b[1;33m75\u001b[0m\u001b[1;33m-8f713978188a\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m40\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;ipython-input-</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">75</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">-8f713978188a&gt;:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">40</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Task(PythonOperator): executar_transformacoes_pyspark>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    }
  ]
}